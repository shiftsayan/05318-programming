{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Loan Prediction\n",
    "This dataset `full_home_loans.csv` is about home loan applications in Washington state, USA, where each row of the dataset is an individual loan application. Your goal in this assignment is to build a machine learning model that can accurately predict whether a given loan application was accepted or rejected. \n",
    "\n",
    "\n",
    "## Part 1: Data Exploration\n",
    "The first few exercises will get you used to looking at the data using `pandas`. Pandas is a widely used library in python for manipulating data. Why? Datasets can consume a _lot_ of space in your computer's memory and traditional python data structures like lists or dictionaries will become painfully slow as we add thousands of rows of data. We use a specialized dataset library `pandas` which has a specialized data structure called a `dataframe` designed to be ultra fast & efficient. Documentation is here: https://pandas.pydata.org/pandas-docs/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # import pandas library\n",
    "df = pd.read_csv('data/home_loans.csv', low_memory=False) # read the csv file into a pandas dataframe object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what kind of data was collected, `pandas` has some handy commands:\n",
    "\n",
    "- `df.head()` will show us the first 5 rows of our dataset. You can also specify the first N rows, like `df.head(18)` will show us the first 18 rows.\n",
    "- `df.sample(10)` will show us 10 randomly sampled rows of our dataset\n",
    "- `df.shape` will tell us how many rows and how many columns are in the dataset\n",
    "- `df.columns` will list the names of all columns in the dataset\n",
    "- `df.describe()` will give you summary statistics about all numerical columns in the dataset\n",
    "\n",
    "### Question 1.A:  How many rows are in this dataset? How many columns?\n",
    "_Double click to write your answer question here. Show your work in code below if applicable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(369281, 27) # df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.B: One of the columns in the dataset is the outcome value for each application, the value we will try to predict. Which column is that?\n",
    "_Double click to write your answer question here. Show your work in code below if applicable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_approved # df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.C: What reasons were given in this dataset for denying a loan application?\n",
    "Hint: There are 3 columns in the dataset that list why a loan was denied. Try looking up the pandas command to list the unique values in a column.\n",
    "\n",
    "_Double click to write your answer question here. Show your work in code below if applicable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Credit application incomplete' 'Collateral'\n",
    " 'Debt-to-income ratio' 'Credit history' 'Unverifiable information'\n",
    " 'Employment history' 'Insufficient cash (downpayment, closing costs)'\n",
    " 'Mortgage insurance denied' 'Other']\n",
    "# pd.unique(df[['denial_reason_name_1', 'denial_reason_name_2', 'denial_reason_name_3']].values.ravel('K'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.D: Given the denial reasons and the columns in this dataset, think about what information you _don't_ have about each application. Rank your top 3 _missing_ pieces of information about each application that could help you better predict the application's loan outcome.\n",
    "_Double click to write your answer question here. Show your work in code below if applicable._\n",
    "#1. Applicant's age\n",
    "#2. Applicant's current debt or credit score\n",
    "#3. Applicant's credit application completion (as a percentage or boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparing Data to Input to a Model\n",
    "Here we'll start using `scikit-learn` which provides simple library calls for most things we'd like to do in a simple machine learning pipeline. If you haven't used `scikit-learn` before this tutorial may be useful to give you a sense of what the library can do: https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "Machine learning models can only understand data that is represented numerically, but lots of the columns in our dataset like \"town_name\" are text _categorical_ data. Meanwhile, many models do better when continous numerical data is within small, consistent ranges, such as all data being between -1, 0 and 1, which is definitely not the case with our thousands of dollars loan units.\n",
    "\n",
    "So first, we will seperate out our samples (called _X_) into features we'd like to include in our model that are categorical or continous so that we can preprocess each appropriately seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn # import scikit-learn\n",
    "from sklearn import preprocessing # import preprocessing utilites\n",
    "\n",
    "features_cat = ['loan_purpose_name', 'applicant_sex_name']\n",
    "features_num = ['loan_amount_000s', 'applicant_income_000s']\n",
    "\n",
    "X_cat = df[features_cat]\n",
    "X_num = df[features_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.A One Hot Encode Categorical Variables\n",
    "Run the following code to one hot encode the categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_cat) # fit the encoder to categories in our data \n",
    "one_hot = enc.transform(X_cat) # transform data into one hot encoded sparse array format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, put the newly encoded sparse array back into a pandas dataframe so that we can use it\n",
    "X_cat_proc = pd.DataFrame(one_hot.toarray(), columns=enc.get_feature_names())\n",
    "X_cat_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.A: In your own words, how is one hot coding tranforming the categorical data? What does the term \"one-hot\" refer to?\n",
    "_Double click to write your answer question here._\n",
    "\n",
    "Categorical data must be encoded numerically in order to be fed in machine learning algorithms. Encoding each category as an integer in one way to do this, but when there is no ordinal relationship, allowing the representation to lean on any such ordinal relationship might be bad. A one-hot encoding represents the categorical variables as binary vectors that are 0 in most dimensions and 1 in a single dimension (hence the name). The index of the one-hot dimension identifies the category of data it represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.B Scaling down continuous numerical data\n",
    "Run the following code to normalize any continous numberical features, such as loan dollar amount, between -1 and 0. This process will ensure that the average of that feature, such as the average amount that a person asks for in loan amount, is scaled to 0. Values less than the average will be negative numbers, and values larger than the average will be positive numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = preprocessing.scale(X_num)\n",
    "X_num_proc = pd.DataFrame(scaled, columns=features_num)\n",
    "X_num_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.C Merge our feature sets into one sample dataset _X_ and fix NaN values\n",
    "Run the code below to combine the numerical and categorical feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_num_proc, X_cat_proc], axis=1, sort=False)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.C The code line below removes any NaN values in our sample with 0. NaNs are missing values that a model won't be able to understand. What is the _semantic_ meaning of replaceing a NaN with 0 for the categorical variables? And for the continous numerical variables? \n",
    "_Double click to write your answer question here._\n",
    "\n",
    "In our dataset, categorical variables are represented as one-hot vectors while continuous variables are represented as a normalized float between -1 and 1. Semantically, replacing NaN with 0 for one-hot vectors implies that the record had no category for that field. One the other hand, replacing NaN with 0 for continuous variables is the same as replacing it with the average, since the data was normalized such that the average was scaled to 0."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = X.fillna(0) # remove NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.D Create our target array _y_ that our model will try to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['loan_approved'] # target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.E Split our data into training, test, and validation sets\n",
    "Run the code below to split the data. Both validation and test sets will be used for testing our model, but use the validation set while you are developing and improving your model, and leave the test for late stage evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_TEMP, y_train, y_TEMP = train_test_split(X, y, test_size=0.30) # split out into training 70% of our data\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_TEMP, y_TEMP, test_size=0.50) # split out into validation 15% of our data and test 15% of our data\n",
    "print(X_train.shape, X_validation.shape, X_test.shape) # print data shape to check the sizing is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.E:  In a  single sentence, what is the difference between train, test, and validation sets?\n",
    "_Double click to write your answer question here._\n",
    "\n",
    "The training set is used to train the actual machine learning model and adjust the parameters. The validation set is used to make sure the model is not overfitting to the training set. The test set is used to measure the final predictive accuracy if our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Developing Models\n",
    "Scikit-learn has a substantial library of different models we can use for classification. Below are implemented two of the most simple classification models, Logistic Regression and Dummy Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# helper method to print basic model metrics\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    print('\\nReport:\\n', classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs').fit(X_train, y_train) # first fit (train) the model\n",
    "y_pred = model.predict(X_validation) # next get the model's predictions for a sample in the validation set\n",
    "metrics(y_validation, y_pred) # finally evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dummy Classifier is a 'dummy' because it is going to use zero machine learning, and simply predict \"approve this loan\" (value 1) for every loan it sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "approve_everyone = DummyClassifier(strategy='constant', constant = 1).fit(X_train, y_train) # first fit (train) the model\n",
    "y_pred_dummy = approve_everyone.predict(X_validation) # next get the model's predictions for a sample in the validation set\n",
    "metrics(y_validation, y_pred_dummy) # finally evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.A: Considering only the data itself, why do Logistic Regression and the Dummy Classifier perform the same? What is the semantic meaning for why Dummy Classifier has such high accuracy?\n",
    "_Double click to write your answer question here._\n",
    "\n",
    "Logistic regression and the dummy classifier perform the same because the dataset is unbalanced and most of the samples in the testing set seem to have the loan approved. Therefore, the dummy classifier, which approves the loan for everyone, also gets 84% accuracy even though it is \"stupid\". Moreover, due to the unbalanced dataset, even the \"smarter\" logistic regression always chooses the more commmon class without actually analyzing the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Your turn!\n",
    "\n",
    "### Task 4.A: Create a new balanced dataset where exactly half of the samples are rejected loan applications and half are accepted loan application.\n",
    "_show your work below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebalance Database\n",
    "# Source: https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets)\n",
    "count_class_0, count_class_1 = df.loan_approved.value_counts()\n",
    "balanced_count = min(count_class_0, count_class_1)\n",
    "\n",
    "# Divide by Class\n",
    "df_class_0 = df[df['loan_approved'] == 0]\n",
    "df_class_1 = df[df['loan_approved'] == 1]\n",
    "\n",
    "# Randomly Undersample\n",
    "df_class_0_under = df_class_0.sample(balanced_count)\n",
    "df_class_1_under = df_class_1.sample(balanced_count)\n",
    "df = pd.concat([df_class_0_under, df_class_1_under], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.B: Below, retry training and evaluating a Logistic regression model on the updated data.\n",
    "_show your work below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Confusion matrix:\n",
    " [[6968 1973]\n",
    " [4405 4768]]\n",
    "\n",
    "Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.61      0.78      0.69      8941\n",
    "           1       0.71      0.52      0.60      9173\n",
    "\n",
    "    accuracy                           0.65     18114\n",
    "   macro avg       0.66      0.65      0.64     18114\n",
    "weighted avg       0.66      0.65      0.64     18114\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.C: Use your own imagination and experimentation to improve predictive performance for this task, modifying the model choices, feature choices, and data processing however you wish.\n",
    "_Important! Your ability to improve the model above the baseline after Task 4.B will count for 10% of this assignment grade, with 5% of that given for modest improvements to performance. Thus while we encourage you to experiment, do not sink excessive time into this task. We will test the performance on our own holdout dataset._\n",
    "\n",
    "_show your work below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/home_loans.csv', low_memory=False) # read the csv file into a pandas dataframe object\n",
    "\n",
    "# Rebalance Database\n",
    "# Source: https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets)\n",
    "count_class_0, count_class_1 = df.loan_approved.value_counts()\n",
    "balanced_count = min(count_class_0, count_class_1)\n",
    "\n",
    "# Divide by Class\n",
    "df_class_0 = df[df['loan_approved'] == 0]\n",
    "df_class_1 = df[df['loan_approved'] == 1]\n",
    "\n",
    "# Randomly Undersample\n",
    "df_class_0_under = df_class_0.sample(balanced_count)\n",
    "df_class_1_under = df_class_1.sample(balanced_count)\n",
    "df = pd.concat([df_class_0_under, df_class_1_under], axis=0)\n",
    "\n",
    "import sklearn # import scikit-learn\n",
    "from sklearn import preprocessing # import preprocessing utilites\n",
    "\n",
    "features_cat = ['loan_purpose_name', 'applicant_sex_name', 'applicant_ethnicity_name', 'co_applicant_ethnicity_name', 'is_hoepa_loan', 'co_applicant_sex_name', 'property_type_name', 'loan_type_name', 'occupied_by_owner', 'applicant_race_name_1', 'co_applicant_race_name_1']\n",
    "features_num = ['loan_amount_000s', 'applicant_income_000s']\n",
    "\n",
    "X_cat = df[features_cat]\n",
    "X_num = df[features_num]\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_cat) # fit the encoder to categories in our data \n",
    "one_hot = enc.transform(X_cat) # transform data into one hot encoded sparse array format\n",
    "\n",
    "# Finally, put the newly encoded sparse array back into a pandas dataframe so that we can use it\n",
    "X_cat_proc = pd.DataFrame(one_hot.toarray(), columns=enc.get_feature_names())\n",
    "X_cat_proc.head()\n",
    "\n",
    "scaled = preprocessing.scale(X_num)\n",
    "X_num_proc = pd.DataFrame(scaled, columns=features_num)\n",
    "X_num_proc.head()\n",
    "\n",
    "X = pd.concat([X_num_proc, X_cat_proc], axis=1, sort=False)\n",
    "X.head()\n",
    "\n",
    "X = X.fillna(0) # remove NaN values\n",
    "\n",
    "y = df['loan_approved'] # target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_TEMP, y_train, y_TEMP = train_test_split(X, y, test_size=0.30) # split out into training 70% of our data\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_TEMP, y_TEMP, test_size=0.50) # split out into validation 15% of our data and test 15% of our data\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# helper method to print basic model metrics\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    print('\\nReport:\\n', classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(10, 8), random_state=1)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_validation)\n",
    "metrics(y_validation, y_pred)\n",
    "\n",
    "'''\n",
    "Confusion matrix:\n",
    " [[6085 2950]\n",
    " [2726 6353]]\n",
    "\n",
    "Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.69      0.67      0.68      9035\n",
    "           1       0.68      0.70      0.69      9079\n",
    "\n",
    "    accuracy                           0.69     18114\n",
    "   macro avg       0.69      0.69      0.69     18114\n",
    "weighted avg       0.69      0.69      0.69     18114\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
